import tensorflow as tf
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split






def func(x):
    return x**2

xDatas=[]
x= torch.tensor(6. , requires_grad= True)
n= 10
eta= .1
for i in range (0, n):
    xDatas.append(x.data.item())
    func(x).backward()
    x.data -= eta * x.grad
    x.grad.zero_()
yDatas=[func(torch.tensor(x).item()) for x in xDatas]

# Plot of y=x**2
xList= torch.linspace(-10, 10, 300)
yList= func(xList)
plt.plot(xList, yList)
plt.scatter(xDatas, yDatas, color= 'red')
plt.xlabel("X")
plt.ylabel("Y")
plt.title("Gradient Descent Visualization")
plt.show()
    



def func(x):
    return x**4 - 8*x**2 - 3*x


xDatas=[]
x= torch.tensor(-1. , requires_grad= True)
n= 1000
eta= .01
gradient_threshold= .01

for i in range (n):
    xDatas.append(x.data.item())
    func(x).backward()
    if abs(x.grad.item()) < gradient_threshold:
        print(f"Converged at step {i}: gradient = {x.grad.item():.6f}")
        break
    with torch.no_grad():
        x.data += eta * x.grad
        x.grad.zero_()
yDatas=[func(torch.tensor(x).item()) for x in xDatas]

# Plot of y=x**2
xList= torch.linspace(-3, 3, 3000)
yList= func(xList)
plt.plot(xList, yList, label= "f(x) = x^4 - 8*x^2 - 3*x")
plt.scatter(xDatas, yDatas, color= 'red')
plt.xlabel("X")
plt.ylabel("Y")
plt.title("Gradient Descent Visualization")
plt.show()
    


class Neuron:
    def __init__(self, n, af='linear'):
        self.w= torch.randn(n, requires_grad= True)
        self.b= torch.randn(1, requires_grad= True)
        self.af= af
    def __call__(self, X):
        z= self.w @ X + self.b
        print(f'w: {self.w}')
        print(f'b: {self.b.item()}')
        print(f'z: {self.w}')
        if self.af == 'linear':
            return self.linear(z)
        elif self.af == 'step':
            return self.step(z)
        elif self.af == 'relu':
            return self.relu(z)
        elif self.af == 'sigmoid':
            return self.sigmoid(z)
        elif self.af == 'tanh':
            return self.tanh(z)
        else:
            raise ValueError(f'Unsopported activation function {self.af}')

    
    def linear(self, x):
        return x
    def step(self, x):
        return (x>0).float()
    def relu(self, x):
        return torch.relu(x)        #return torch.maximum(x, torch.tensor(0.0)) 
    def sigmoid(self, x):
        return torch.sigmoid(x)     #return 1 / (1 + torch.exp(-x))
    def tanh(self, x):
        return torch.tanh(x)


x= torch.tensor([1, 5, 8, .65, 6])
y = torch.tensor([.4])
neuron1 = Neuron(5,'tanh')
y_pred = neuron1(x)
mse_loss= (y_pred - y) ** 2
print(f'y_pred: {y_pred.item()}')
print(f'mse_loss: {mse_loss.item()}')


class Neuron:
    def __init__(self, n_x, n_y, af='linear'):
        self.w= torch.randn(n_y, n_x, requires_grad= True)
        self.b= torch.randn(n_y, requires_grad= True)
        self.af= af
    
    def forward(self, X):
        z= self.w @ X + self.b
        print(f'w: {self.w.detach().numpy()}')
        print(f'b: {self.b.detach().numpy()}')
        print(f'z: {z.detach().numpy()}')
        
        if self.af == 'linear':
            return self.linear(z)
        elif self.af == 'step':
            return self.step(z)
        elif self.af == 'relu':
            return self.relu(z)
        elif self.af == 'sigmoid':
            return self.sigmoid(z)
        elif self.af == 'tanh':
            return self.tanh(z)
        else:
            raise ValueError(f'Unsopported activation function {self.af}')        
    
    def __call__(self, X, y=None, lr=None):
        yp = self.forward(X)
        if y is not None:
            print(f"yt (y true) : {y.detach().numpy()}")
        print(f"yp (y predict) : {yp.detach().numpy()}")

        if y is not None and lr is not None:
            loss= self.compute_loss(yp, y)
            loss.backward()
            self.update_params(lr)
            return yp , loss
        else:
            return yp
            

    def compute_loss(self, yp, yt):
        return ((yp - yt)**2).mean()
        
    def linear(self, x):
        return x
    def step(self, x):
        return (x>0).float()
    def relu(self, x):
        return torch.relu(x)        #return torch.maximum(x, torch.tensor(0.0)) 
    def sigmoid(self, x):
        return torch.sigmoid(x)     #return 1 / (1 + torch.exp(-x))
    def tanh(self, x):
        return torch.tanh(x)

    def update_params(self, lr):
        with torch.no_grad():
            self.w -= lr * self.w.grad
            self.b -= lr * self.b.grad
            self.zero_grad()
    def zero_grad(self):
        if self.w.grad is not None:
            self.w.grad.zero_()
        if self.b.grad is not None:
            self.b.grad.zero_()        


x= torch.tensor([1, 5, 8, .65, 6, 10])
y = torch.tensor([6., 3.2])
neuron1 = Neuron(6, 2, 'linear')
y_pred , loss= neuron1(x, y, 0.1)



print(y_pred , loss)


import torch.nn as nn


# x= torch.tensor([1, 5, 8, .65, 6, 10])
# y = torch.tensor([6., 3.2])

x= torch.randn(200, 4)
y = torch.randn(200, 2)

model = nn.Linear(in_features=4, out_features=2)
print(f'Bias: {model.bias.detach().numpy()}\nWeights: {model.weight.detach().numpy()}')

yp= model(x)
# print(f'y predict: {yp.detach().numpy()}')

loss_fun = nn.MSELoss()
mse = loss_fun(yp, y)
print(f'MSE : {mse}')



from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

housing= fetch_california_housing()
m, n= housing.data.shape

X = torch.tensor(housing.data, dtype=torch.float32)
y = torch.tensor(housing.target, dtype=torch.float32).reshape(-1, 1)

scaler = StandardScaler()
X = torch.tensor(scaler.fit_transform(housing.data), dtype=torch.float32)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=42)


model = nn.Linear(n, 1)
loss_fun = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)

for epoch in range(100):
    yp = model(X_train)
    mse = loss_fun(yp, y_train)
    # print(f"{model.bias.detach().numpy()}  {model.weight.detach().numpy()}   {mse.item()}")
    # print(f"{mse.item()}")
    mse.backward()
    optimizer.step()
    optimizer.zero_grad()

y_predict = model(X_test)
l1_loss = nn.functional.l1_loss(y_predict, y_test)

l1_loss.item()





from sklearn.datasets import load_sample_images


dataset= np.array(load_sample_images().images, dtype=np.float32)
batch_size, height, width, channels = dataset.shape

filters_test = np.zeros(shape= (7, 7, channels, 2), dtype=np.float32)
filters_test[:, 3, :, 0] = 1 # vertical line  # All rows, column 3, all channels, filter 0
filters_test[3, :, :, 1] = 1 # horizontal line  # Row 3, all columns, all channels, filter 1

convolution = tf.nn.conv2d(dataset, filters_test, padding= "SAME", strides=1)
convolution_np = convolution.numpy()


convolution_np[1]


fix, axes = plt.subplots(batch_size, 4, figsize =(16, 10))  # figsize=(16, 4 * batch_size)
for i in range(batch_size):
    axes[i, 0].imshow(dataset[i].astype(np.int32))  # astype(np.uint8)
    axes[i, 0].set_title(f'Original Image {i+1}')
    axes[i, 0].axis('off')



    
    # Filter 1: Vertical line detector
    axes[i, 1].imshow(convolution_np[i, :, :, 0], cmap='gray')
    axes[i, 1].set_title(f'Vertical Filter Result {i+1}')
    axes[i, 1].axis('off')
    
    # Filter 2: Horizontal line detector
    axes[i, 2].imshow(convolution_np[i, :, :, 1], cmap='gray')
    axes[i, 2].set_title(f'Horizontal Filter Result {i+1}')
    axes[i, 2].axis('off')
    
    # Combined visualization
    axes[i, 3].imshow(np.maximum(convolution_np[i, :, :, 0], convolution_np[i, :, :, 1]), cmap='gray')
    axes[i, 3].set_title(f'Combined Result {i+1}')
    axes[i, 3].axis('off')

plt.tight_layout()
plt.show()

# Display the filters
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Display filter patterns
axes[0].imshow(filters_test[:, :, 0, 0], cmap='gray')
axes[0].set_title('Vertical Line Filter (Channel 1)')
axes[0].axis('off')

axes[1].imshow(filters_test[:, :, 0, 1], cmap='gray')
axes[1].set_title('Horizontal Line Filter (Channel 1)')
axes[1].axis('off')

# Show what the filters look like (for first channel only)
filter_display = np.zeros((7, 14))
filter_display[:, :7] = filters_test[:, :, 0, 0]
filter_display[:, 7:] = filters_test[:, :, 0, 1]
axes[2].imshow(filter_display, cmap='gray')
axes[2].set_title('Both Filters (Vertical | Horizontal)')
axes[2].axis('off')

plt.tight_layout()
plt.show()

# Print some statistics
print("\n--- Convolution Results Statistics ---")
print(f"Output shape: {convolution_np.shape}")
print(f"Min value in vertical filter output: {convolution_np[:, :, :, 0].min():.2f}")
print(f"Max value in vertical filter output: {convolution_np[:, :, :, 0].max():.2f}")
print(f"Mean value in vertical filter output: {convolution_np[:, :, :, 0].mean():.2f}")
print(f"Min value in horizontal filter output: {convolution_np[:, :, :, 1].min():.2f}")
print(f"Max value in horizontal filter output: {convolution_np[:, :, :, 1].max():.2f}")
print(f"Mean value in horizontal filter output: {convolution_np[:, :, :, 1].mean():.2f}")

# Alternative: Interactive visualization with larger images
plt.figure(figsize=(20, 10))

# For the first image only, show detailed results
img_idx = 0  # You can change this to 1 for the second image

# Original image
plt.subplot(2, 4, 1)
plt.imshow(dataset[img_idx].astype(np.uint8))
plt.title('Original Image')
plt.axis('off')

# Original image (grayscale for comparison)
plt.subplot(2, 4, 2)
gray_img = np.mean(dataset[img_idx], axis=2)
plt.imshow(gray_img, cmap='gray')
plt.title('Original (Grayscale)')
plt.axis('off')

# Vertical filter result
plt.subplot(2, 4, 3)
plt.imshow(convolution_np[img_idx, :, :, 0], cmap='gray')
plt.title('Vertical Line Detection')
plt.axis('off')

# Horizontal filter result
plt.subplot(2, 4, 4)
plt.imshow(convolution_np[img_idx, :, :, 1], cmap='gray')
plt.title('Horizontal Line Detection')
plt.axis('off')

# Combined result
plt.subplot(2, 4, 5)
combined = np.sqrt(convolution_np[img_idx, :, :, 0]**2 + convolution_np[img_idx, :, :, 1]**2)
plt.imshow(combined, cmap='gray')
plt.title('Combined Edge Detection')
plt.axis('off')

# Thresholded vertical result
plt.subplot(2, 4, 6)
threshold = np.percentile(convolution_np[img_idx, :, :, 0], 90)
vertical_thresh = np.where(convolution_np[img_idx, :, :, 0] > threshold, 1, 0)
plt.imshow(vertical_thresh, cmap='gray')
plt.title(f'Vertical (Top 10%: >{threshold:.1f})')
plt.axis('off')

# Thresholded horizontal result
plt.subplot(2, 4, 7)
threshold = np.percentile(convolution_np[img_idx, :, :, 1], 90)
horizontal_thresh = np.where(convolution_np[img_idx, :, :, 1] > threshold, 1, 0)
plt.imshow(horizontal_thresh, cmap='gray')
plt.title(f'Horizontal (Top 10%: >{threshold:.1f})')
plt.axis('off')

# Edges overlay on original
plt.subplot(2, 4, 8)
plt.imshow(dataset[img_idx].astype(np.uint8))
# Create edge mask
edge_mask = np.maximum(vertical_thresh, horizontal_thresh)
# Overlay edges in red
overlay = np.copy(dataset[img_idx])
overlay[edge_mask == 1] = [255, 0, 0]  # Red color for edges
plt.imshow(overlay.astype(np.uint8))
plt.title('Edges Overlay (Red)')
plt.axis('off')

plt.tight_layout()
plt.show()



