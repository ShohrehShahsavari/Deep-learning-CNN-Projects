import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras as keras
import warnings
warnings.filterwarnings('ignore')
from keras.datasets import cifar10
import cv2





(X_train, y_train), (X_test, y_test) = cifar10.load_data()#(label_mode='coarse')


X_train.shape





unique_labels = np.unique(y_train)
print(f"Unique numerical labels in y_train: {unique_labels}")
print(f"Number of unique labels: {len(unique_labels)}\n")

# Class names
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
for i, name in enumerate(class_names):
    print(f"{i}: {name}")


fix, axes= plt.subplots(2, 5, figsize=(10, 5))
axes = axes.flatten()
for i in range(10):
    axes[i].imshow(X_train[i])
    axes[i].set_title(f"{class_names[int(y_train[i])]}")
plt.tight_layout()
plt.show()





# # Pre-allocate the array with the target shape
import gc
X_train_s = X_train[:1000]
y_train = y_train[:1000]

X_test_s  = X_test[:200]
y_test = y_test[:200]


X_train_s_resized = np.zeros((1000, 224, 224, 3), dtype=np.uint8)
# Fill it incrementally
for i in range(len(X_train_s)):
    X_train_s_resized[i] = cv2.resize(X_train_s[i], (224, 224))
# Free original
del X_train_s
gc.collect()

# Pre-allocate the array with the target shape
X_test_s_resized = np.zeros((200, 224, 224, 3), dtype=np.uint8)
# Fill it incrementally
for i in range(len(X_test_s)):
    X_test_s_resized[i] = cv2.resize(X_test_s[i], (224, 224))
# Free original
del X_train
gc.collect()

# Convert to float32 and normalize
X_train = X_train_s_resized.astype(np.float32) / 255.0
X_test = X_test_s_resized.astype(np.float32) / 255.0

# Free uint8 arrays
del X_train_s_resized, X_test_s_resized
gc.collect()








# X_train = X_train[:2000].astype(np.float32) / 255.0
# y_train = y_train[:2000]

# X_test = X_test[:500].astype(np.float32) / 255.0
# y_test = y_test[:500]


data_augmentation = keras.Sequential([
    # keras.layers.Resizing(224, 224)#,
    keras.layers.RandomFlip('horizontal'),
    keras.layers.RandomTranslation(
        height_factor=0.1,
        width_factor=0.1
    ),
    keras.layers.RandomBrightness(factor=0.2),
    keras.layers.RandomContrast(factor=0.2)

], name="data_augmentation")


# input_layer = keras.layers.Input(shape=(32, 32, 3))
input_layer = keras.layers.Input(shape=(224, 224, 3))
X = data_augmentation(input_layer)

C1_layer = keras.layers.Conv2D(filters=64, kernel_size=(7, 7), strides=2, padding='SAME', activation= 'relu')(X)
pool1_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='SAME')(C1_layer)

C2_layer = keras.layers.Conv2D(filters=64, kernel_size=(1, 1), activation= 'relu')(pool1_layer)

C3_layer = keras.layers.Conv2D(filters=192, kernel_size=(3, 3), padding='SAME', activation= 'relu')(C2_layer)
pool3_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='SAME')(C3_layer)
## output_size(28, 28, 192)


## Inception 3a
#Branch 1
I3a_1 = keras.layers.Conv2D(filters= 64, kernel_size=(1, 1), activation= 'relu')(pool3_layer)

#Branch 2
I3a_2 = keras.layers.Conv2D(filters=96, kernel_size=(1, 1), activation='relu')(pool3_layer)
I3a_2 = keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='SAME')(I3a_2)

#Branch3
I3a_3 = keras.layers.Conv2D(filters=16 ,kernel_size=(1, 1), activation='relu')(pool3_layer)
I3a_3 = keras.layers.Conv2D(filters=32 ,kernel_size=(5, 5), activation='relu', padding='SAME')(I3a_3)

#Branch 4
I3a_4 = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=1, padding='SAME')(pool3_layer)
I3a_4 = keras.layers.Conv2D(filters=32 ,kernel_size=(1, 1), activation='relu')(I3a_4)

Inception_3a = keras.layers.Concatenate(axis=-1)([I3a_1, I3a_2, I3a_3, I3a_4])
## output_size(28, 28, (64 + 128 + 32 + 32 = 256 channels))


## Inception 3b
#Branch 1
I3b_1 = keras.layers.Conv2D(filters= 128, kernel_size=(1, 1), activation= 'relu')(Inception_3a)

#Branch 2
I3b_2 = keras.layers.Conv2D(filters=128, kernel_size=(1, 1), activation='relu')(Inception_3a)
I3b_2 = keras.layers.Conv2D(filters=192, kernel_size=(3, 3), activation='relu', padding='SAME')(I3b_2)

#Branch3
I3b_3 = keras.layers.Conv2D(filters=32 ,kernel_size=(1, 1), activation='relu')(Inception_3a)
I3b_3 = keras.layers.Conv2D(filters=96 ,kernel_size=(5, 5), activation='relu', padding='SAME')(I3b_3)

#Branch 4
I3b_4 = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=1, padding='SAME')(Inception_3a)
I3b_4 = keras.layers.Conv2D(filters=64 ,kernel_size=(1, 1), activation='relu')(I3b_4)

Inception_3b = keras.layers.Concatenate(axis=-1)([I3b_1, I3b_2, I3b_3, I3b_4])
## output_size(28, 28, (128 + 192 + 96 + 64 = 480 channels))


pool4_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), padding='SAME', strides=2)(Inception_3b)
## output_size(14, 14, 480)


## Inception 4a
#Branch 1
I4a_1 = keras.layers.Conv2D(filters= 192, kernel_size=(1, 1), activation= 'relu')(pool4_layer)

#Branch 2
I4a_2 = keras.layers.Conv2D(filters=96, kernel_size=(1, 1), activation='relu')(pool4_layer)
I4a_2 = keras.layers.Conv2D(filters=208, kernel_size=(3, 3), activation='relu', padding='SAME')(I4a_2)

#Branch3
I4a_3 = keras.layers.Conv2D(filters=16 ,kernel_size=(1, 1), activation='relu')(pool4_layer)
I4a_3 = keras.layers.Conv2D(filters=48 ,kernel_size=(5, 5), activation='relu', padding='SAME')(I4a_3)

#Branch 4
I4a_4 = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=1, padding='SAME')(pool4_layer)
I4a_4 = keras.layers.Conv2D(filters=64 ,kernel_size=(1, 1), activation='relu')(I4a_4)

Inception_4a = keras.layers.Concatenate(axis=-1)([I4a_1, I4a_2, I4a_3, I4a_4])
## output_size(14, 14, (192 + 208 + 48 + 64 = 512 channels))


flatten_layer = keras.layers.GlobalAveragePooling2D()(Inception_4a)
Dens1_layer = keras.layers.Dropout(rate = 0.4)(flatten_layer)


output_layer = keras.layers.Dense(20, activation= 'softmax')(Dens1_layer)

model = keras.Model(input_layer, output_layer)
model.compile(
    optimizer= keras.optimizers.Adam(learning_rate=0.0001),
    loss= 'sparse_categorical_crossentropy',
    metrics=['sparse_categorical_accuracy']
)


model.summary()


callbacks = [
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,           # Reduce LR by half
        patience=3,           # Wait 3 epochs with no improvement
        min_lr=1e-6           # Minimum learning rate
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,          # Stop after 10 epochs without improvement
        restore_best_weights=True
    )
]
GoogLe_model = model.fit(X_train, y_train, epochs=10, batch_size= 32, validation_data=(X_test, y_test), callbacks=callbacks)


plt.plot(GoogLe_model.history['loss'], color='blue')
plt.plot(GoogLe_model.history['val_loss'], color='red')


GoogLe_model.history['loss']


plt.plot(GoogLe_model.history['sparse_categorical_accuracy'], color='blue')
plt.plot(GoogLe_model.history['val_sparse_categorical_accuracy'], color='red')





X_train_s = X_train[:2000]
y_train_s = y_train[:2000]

X_test_s  = X_test[:500]
y_test_s  = y_test[:500]

X_train_s = X_train_s.astype(np.float32) / 255.0
X_test_s = X_test_s.astype(np.float32) / 255.0

X_train_s = np.array([cv2.resize(img, (140, 140)) for img in X_train_s])
X_test_s = np.array([cv2.resize(img, (140, 140)) for img in X_test_s])


input_layer = keras.layers.Input(shape=(140, 140, 3))

# X = data_augmentation(input_layer)

C1_layer = keras.layers.Conv2D(filters=64, kernel_size=(7, 7), strides=2, padding='SAME', activation= 'relu')(X)
pool1_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='SAME')(C1_layer)

C2_layer = keras.layers.Conv2D(filters=64, kernel_size=(1, 1), activation= 'relu')(pool1_layer)

C3_layer = keras.layers.Conv2D(filters=192, kernel_size=(3, 3), padding='SAME', activation= 'relu')(C2_layer)
pool3_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='SAME')(C3_layer)
## output_size(28, 28, 192)


## Inception 3a
#Branch 1
I3a_1 = keras.layers.Conv2D(filters= 64, kernel_size=(1, 1), activation= 'relu')(pool3_layer)

#Branch 2
I3a_2 = keras.layers.Conv2D(filters=96, kernel_size=(1, 1), activation='relu')(pool3_layer)
I3a_2 = keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='SAME')(I3a_2)

#Branch3
I3a_3 = keras.layers.Conv2D(filters=16 ,kernel_size=(1, 1), activation='relu')(pool3_layer)
I3a_3 = keras.layers.Conv2D(filters=32 ,kernel_size=(5, 5), activation='relu', padding='SAME')(I3a_3)

#Branch 4
I3a_4 = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=1, padding='SAME')(pool3_layer)
I3a_4 = keras.layers.Conv2D(filters=32 ,kernel_size=(1, 1), activation='relu')(I3a_4)

Inception_3a = keras.layers.Concatenate(axis=-1)([I3a_1, I3a_2, I3a_3, I3a_4])
## output_size(28, 28, (64 + 128 + 32 + 32 = 256 channels))


## Inception 3b
#Branch 1
I3b_1 = keras.layers.Conv2D(filters= 128, kernel_size=(1, 1), activation= 'relu')(Inception_3a)

#Branch 2
I3b_2 = keras.layers.Conv2D(filters=128, kernel_size=(1, 1), activation='relu')(Inception_3a)
I3b_2 = keras.layers.Conv2D(filters=192, kernel_size=(3, 3), activation='relu', padding='SAME')(I3b_2)

#Branch3
I3b_3 = keras.layers.Conv2D(filters=32 ,kernel_size=(1, 1), activation='relu')(Inception_3a)
I3b_3 = keras.layers.Conv2D(filters=96 ,kernel_size=(5, 5), activation='relu', padding='SAME')(I3b_3)

#Branch 4
I3b_4 = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=1, padding='SAME')(Inception_3a)
I3b_4 = keras.layers.Conv2D(filters=64 ,kernel_size=(1, 1), activation='relu')(I3b_4)

Inception_3b = keras.layers.Concatenate(axis=-1)([I3b_1, I3b_2, I3b_3, I3b_4])
## output_size(28, 28, (128 + 192 + 96 + 64 = 480 channels))


pool4_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), padding='SAME', strides=2)(Inception_3b)
## output_size(14, 14, 480)


## Inception 4a
#Branch 1
I4a_1 = keras.layers.Conv2D(filters= 192, kernel_size=(1, 1), activation= 'relu')(pool4_layer)

#Branch 2
I4a_2 = keras.layers.Conv2D(filters=96, kernel_size=(1, 1), activation='relu')(pool4_layer)
I4a_2 = keras.layers.Conv2D(filters=208, kernel_size=(3, 3), activation='relu', padding='SAME')(I4a_2)

#Branch3
I4a_3 = keras.layers.Conv2D(filters=16 ,kernel_size=(1, 1), activation='relu')(pool4_layer)
I4a_3 = keras.layers.Conv2D(filters=48 ,kernel_size=(5, 5), activation='relu', padding='SAME')(I4a_3)

#Branch 4
I4a_4 = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=1, padding='SAME')(pool4_layer)
I4a_4 = keras.layers.Conv2D(filters=64 ,kernel_size=(1, 1), activation='relu')(I4a_4)

Inception_4a = keras.layers.Concatenate(axis=-1)([I4a_1, I4a_2, I4a_3, I4a_4])
## output_size(14, 14, (192 + 208 + 48 + 64 = 512 channels))


flatten_layer = keras.layers.GlobalAveragePooling2D()(Inception_4a)
Dens1_layer = keras.layers.Dropout(rate = 0.4)(flatten_layer)


output_layer = keras.layers.Dense(20, activation= 'softmax')(Dens1_layer)

model = keras.Model(input_layer, output_layer)
model.compile(
    optimizer= keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),
    loss= 'sparse_categorical_crossentropy',
    metrics=['sparse_categorical_accuracy']
)

callbacks = [
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,           # Reduce LR by half
        patience=3,           # Wait 3 epochs with no improvement
        min_lr=1e-6           # Minimum learning rate
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,          # Stop after 10 epochs without improvement
        restore_best_weights=True
    )
]
GoogLeNet_model = model.fit(X_train_s, y_train_s, epochs=10, validation_data=(X_test_s, y_test_s), callbacks=callbacks)


plt.plot(GoogLeNet_model.history['loss'], color='blue')
plt.plot(GoogLeNet_model.history['val_loss'], color='red')
plt.title('GoogLeNet Training progress 2000*140*140*3')
plt.legend(['Training loss', 'Validation loss'], loc='best', fontsize=12)
plt.tight_layout()
plt.show()


plt.plot(GoogLeNet_model.history['sparse_categorical_accuracy'], color='green')
plt.plot(GoogLeNet_model.history['val_sparse_categorical_accuracy'], color='yellow')
plt.title('GoogLeNet Training progress 2000*140*140*3')
plt.legend(['Training accuracy', 'Validation accuracy'], loc='best', fontsize=12)
plt.tight_layout()
plt.show()


model.summary()


import visualkeras
visualkeras.layered_view(model)



