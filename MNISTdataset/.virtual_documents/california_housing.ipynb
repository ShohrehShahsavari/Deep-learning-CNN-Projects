import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras as layers
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

housing= fetch_california_housing()



housing.data.shape[1]


housing.target.shape


X = pd.DataFrame(housing.data)
y = pd.DataFrame(housing.target)

scaler = StandardScaler()
X = scaler.fit_transform(X)
y = scaler.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split (X, y, train_size= .8, random_state= 42)


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

layer1 = layers.Dense(300, activation= 'relu', input_shape= (housing.data.shape[1],))
layer2 = layers.Dense(100, activation= 'relu')
layer3 = layers.Dense(1)

dnn_model = keras.Sequential([layer1, layer2, layer3])
# optimizer= ['adam', 'rmsprop', 'sgd']
# Or from tensorflow.keras.optimizers import SGD
# optimizer = SGD(learning_rate=0.01, momentum=0.9)
# loss= ['mean_squared_error', 'mean_absolute_error', 'huber_loss', 'mean_squared_logarithmic_error']
# metrics=['mse', 'mse']
dnn_model.compile(optimizer= 'adam',loss= 'mean_squared_error', metrics=['mae']) 
history = dnn_model.fit(X_train, y_train, epochs= 50, batch_size= 50, validation_data = (X_test, y_test)) #40000 // (len(X_train) // 50), batch_size= 50, validation_data = (X_test, y_test))


test_loss, test_mse = dnn_model.evaluate(X_test, y_test)
y_pred = dnn_model.predict(X_test)



